NOTES REGARDING THE STRUCTURING OF THE INTERPRETING THE SCHEMA FOR FILE WRAPPERS (JSON, TABULAR)


What I want to distinguish:
    1. - how will hooks work -> (when does an object need to re-check the schema to see for updates)
    2. - seperation of schema related work from object wrappers

1.  We need hooks -> looking at schema.rules.dataset_metadata we get selectors like:
                    - path == "/dataset_description.json"
                    - '!exists("CITATION.cff", "dataset")'
    In this instance the metadata for dataset_description depends on the parameter of another file (in this case the "exists"
    parameter of CITATION.cff)

    Hooks enable a callback on the relevant schema, to re-evaluate if the parameter change now changes the valid metadata

Design choices of hooks:
    An attribute with callbacks, must remember: the object which depends on it 

    Most reliable method: when initially checking the schema for a file, put callbacks on all selectors it faces
    VERY MEMORY INTENSIVE, you'd get huge lists.. (for stuff like datatype == "eeg", every file would set a callback on every datatype ==
    for that selector...)

    FIX 1: dont have callbacks point to selector, rather just an object. I.e. datatype wouldn't have a list for each selector callback
    for a file, but just 1 being that the file needs to recheck the schema

    POSSIBILITY 1:
        hardcode certain aspects to reduce reliance on intricate callback webs:

        when a filename gets changed (path, entities, suffix, datatype, etc..) rerun schema on self AND ALL children


DIFFICULTY: 
    How to assign callbacks?

    the way selectors work, is given a dataset core they evaluate to true or false...
    can we along the way intercept those which reflect properties to be callbacked?
    i.e. "==" or (operator.eq) is not, but sidecar.attribute is, in this case for attribute

    possibly: have the fields and eval functions hardcoded? i.e the get_property function (".") could have a value 
    passed to add callbacks


2. Should I seperate all schema related operations into a seperate file
i.e. for JSONfile it currently has functions to call the schema, clean its output
Should I really seperate and make JSON file just have a function which takes in a dictionary? 
    maybe yes, this could seperate and make their roles more clear

    would also allow for more refined parsing elegantly, i.e. only parse the relevant datatype
    or setting metadata values already for stuff which has enums with 1 options etc..


this looks at the bigger question of when should the schema be evaluated and looked at?

    When created? -> probably not.. sometimes schema depends on sidecars, other files etc..
                        so the file needs to be linked to the dataset_tree so it can access those
    After "whole initialisation" ? -> probably yes. Once all files are created and linked in the dataset_tree
    In this case we can set up this step in a _post_init_ method(), which gets called by a dataset_tree when it is linked

    brings a bit of a question for support of unlinked files

    i.e. if I user instantiates a subject, or even datatype folder without linking it to a dataset..
    should this be supported? -it technically could...

One last remark:
    treat agnostic files differently
    They should all be initialised before parsing, and adding callbacks. Lots of interconnected dependencies